import numpy as np

## TensorFlow Only ##
# import tensorflow as tf
# import keras
# from model_tensorflow import make_model


## Pytorch Only ##
import torch
from model_pytorch import make_model, ExpertModel
from torch.utils.data import DataLoader


def action_to_one_hot(env, action):
    action_vec = np.zeros(env.action_space.n)
    action_vec[action] = 1
    return action_vec    


def generate_episode(env, policy):
    """Collects one rollout from the policy in an environment. The environment
    should implement the OpenAI Gym interface. A rollout ends when done=True. The
    number of states and actions should be the same, so you should not include
    the final state when done=True.

    Args:
    env: an OpenAI Gym environment.
    policy: The output of a deep neural network
    Returns:
    states: a list of states visited by the agent.
    actions: a list of actions taken by the agent. For tensorflow, it will be 
        helpful to use a one-hot encoding to represent discrete actions. The actions 
        that you return should be one-hot vectors (use action_to_one_hot()).
        For Pytorch, the Cross-Entropy Loss function will integers for action
        labels.
    rewards: the reward received by the agent at each step.
    """

    softmax = torch.nn.Softmax(dim=-1)
    state = torch.tensor(env.reset(), dtype=torch.float)
    terminated = False
    state_arr, action_arr, reward_arr = [], [], []
    while not terminated:
        state_arr.append(state.detach().numpy())

        action_prob = softmax(policy(state))
        action = np.argmax(action_prob.detach().numpy())
        action_arr.append(action)

        state, reward, terminated, _ = env.step(action)
        state = torch.tensor(state, dtype=torch.float)
        reward_arr.append(reward)

    return np.array(state_arr), np.array(action_arr), np.array(reward_arr)
        
def relabel_episodes(policy, states):
    action_arr = []
    for state in states:
        state = torch.tensor(state, dtype=torch.float)
        action_prob = policy(state)
        action = np.argmax(action_prob.detach().numpy())
        action_arr.append(action)
    
    return np.array(action_arr)

class Imitation():
    
    def __init__(self, env, num_episodes, expert_file):
        self.env = env
        self._train_states, self._train_actions = None, None

        # TensorFlow Only #
        # self.expert = tf.keras.models.load_model(expert_file)
        
        # Pytorch Only #
        self.expert = ExpertModel()
        self.expert.load_state_dict(torch.load(expert_file))
        self.expert.eval()
        
        self.num_episodes = num_episodes
        
        self.model = make_model()
        
    def generate_behavior_cloning_data(self):
        self._train_states = []
        self._train_actions = []
        for _ in range(self.num_episodes):
            states, actions, _ = generate_episode(self.env, self.expert)
            self._train_states.extend(states)
            self._train_actions.extend(actions)
        self._train_states = np.array(self._train_states)
        self._train_actions = np.array(self._train_actions)
        
    def generate_dagger_data(self):
        # You should collect states and actions from the student policy
        # (self.model), and then relabel the actions using the expert policy.
        if self._train_states is None or self._train_actions is None:
            self._train_states = np.empty((0,self.env.observation_space.shape[0]))
            self._train_actions = np.empty((0,))
        for _ in range(self.num_episodes):
            states, actions, _ = generate_episode(self.env, self.model)
            actions = relabel_episodes(self.expert, states)
            self._train_states = np.append(self._train_states, states, axis=0)
            self._train_actions = np.append(self._train_actions, actions)

        
    def train(self, num_epochs=1, batch_size=64, mode="behavior cloning"):
        """
        Train the model on data generated by the expert policy.
        Use Cross-Entropy Loss and a batch size of 64 when
        performing updates.
        Args:
            num_epochs: number of epochs to train on the data generated by the expert.
        Return:
            loss: (float) final loss of the trained policy.
            acc: (float) final accuracy of the trained policy
        """

        criterion = torch.nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(self.model.parameters())

        if mode == "behavior cloning":
            # print("BC")
            self.generate_behavior_cloning_data()
        else:
            # print("DAGGER")
            self.generate_dagger_data()

        train_set = np.concatenate((self._train_states, self._train_actions.reshape(-1,1)), axis=1)
        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)

        for epoch in range(num_epochs):
            running_loss = 0
            correct = 0
            for i, data in enumerate(train_loader, 0):

                x_batch = torch.tensor(data[:,:-1], dtype=torch.float)
                y_batch = torch.tensor(data[:,-1], dtype=torch.long)

                optimizer.zero_grad()
                yhat = self.model(x_batch)
                loss = criterion(yhat, y_batch)
                loss.backward()
                optimizer.step()

                correct += (torch.argmax(yhat, dim=1) == y_batch).float().sum()
                running_loss += loss.item()

            acc = correct / len(train_set)
            print('(%d) loss= %.3f; running_loss= %.3f; accuracy = %.1f%%' % (epoch, loss, running_loss, 100 * acc))
        
        return loss.item(), acc.item()



    def evaluate(self, policy, n_episodes=50):
        rewards = []
        for i in range(n_episodes):
            _, _, r = generate_episode(self.env, policy)
            rewards.append(sum(r))
        r_mean = np.mean(rewards)
        return r_mean
    
if __name__ == "__main__":
    import gym
    env = gym.make('CartPole-v0')
    imit = Imitation(env, 100, 'expert_torch.pt')
    imit.train(num_epochs=15, mode="dagger")